[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A curious mathematician",
    "section": "",
    "text": "Bio\nMy name is Niclas Richter, a mathematician by training, currently working as a Data Expert in the field of official statistics. I am passionate about exploring computational methods and utilizing quantitative approaches to solve diverse problems, with expertise in Julia, R, and Python. Generally, I am an inquisitive person, which is why I’ve named this website ‘A Curious Mathematician’.\nI hold a degree (B.Sc.) in Mathematics after studying at TU Dresden and the Hebrew University of Jerusalem.\n\n\nAbout\nThe website is hosted on GitHub Pages with Quarto. All contents is published under the CC BY 4.0 licence and the code is licensed under the MIT Licence."
  },
  {
    "objectID": "blog_posts/handout_berlin_sep_2023.html",
    "href": "blog_posts/handout_berlin_sep_2023.html",
    "title": "The answer to an ever repeating question",
    "section": "",
    "text": "Everyone who studies math or has studied math in the past, has heard the question \\(\\aleph_0\\)-times: “What are you going to do with a degree in math?”. I gave an presentation about it in a German school in Berlin. The slides and the handout (in German) can be found here:\nHandout\nSlides"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "On this blog I write about different methodologies on working with data and computation. Personally I’m not so interested in the results, but rather the way they are obtained.\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Wikipedia page views with seasonal ARIMA models\n\n\n\n\n\n\nNiclas\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create a choroplath map in R\n\n\n\n\n\n\nNiclas\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to solve a zero sum game using linear programming\n\n\n\n\n\n\nNiclas\n\n\nJul 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe answer to an ever repeating question\n\n\n\n\n\n\nNiclas\n\n\nSep 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Niclas Richter, a mathematician by training, currently working as a Data Expert in the field of official statistics. I am passionate about exploring computational methods and utilizing quantitative approaches to solve diverse problems, with expertise in Julia, R, and Python. Generally, I am an inquisitive person, which is why I’ve named this website ‘A Curious Mathematician’.\nI hold a degree (B.Sc.) in Mathematics after studying at TU Dresden and the Hebrew University of Jerusalem.\nThe website is hosted on GitHub Pages with Quarto. All contents is published under the CC BY 4.0 licence and the code is licensed under the MIT Licence."
  },
  {
    "objectID": "blog_posts/LP_zero_sum_games.html",
    "href": "blog_posts/LP_zero_sum_games.html",
    "title": "How to solve a zero sum game using linear programming",
    "section": "",
    "text": "My last undergraduate course in mathematics was about Game Theory, which was quite some time ago. One of the lectures covered Neumann’s Minmax-Theorem, and during this session, the lecturer drew a picture on the blackboard to illustrate how to find the Nash Equilibrium of a zero-sum game. The drawing consisted of a set of linear inequalities, and he moved his hands along the edges of a simplex. While everyone in the lecture hall, including myself, was trying to grasp the explanation, it suddenly clicked, “Isn’t this just linear programming?!?”. Well… I was thinking with mouth moving… At that moment, the entire lecture hall turned to look at me, but the lecturer simply replied with a straightforward “yes.”\nNevertheless, it took quite some time until I stumbled upon a resource that explained the connection between game theory and linear programming, or more generally, mathematical optimization. I found the following statement in [1]:\nTheorem Given a zero-sum matrix game with payoff matrix \\(A\\) is an \\(n\\times m\\) matrix with real entries and \\[ \\hat{X} := \\{ x \\in \\mathbb{R}^n \\| \\sum_{i=1}^n x_i = 1, x \\geq 0 \\}, \\\\ \\hat{Y} := \\{ y \\in \\mathbb{R}^m \\| \\sum_{i=1}^m y_i = 1, y \\geq 0\\}\\] as the sets of mixed strategies. The payoff function is for both players is given by \\[ \\theta(x,y) = x^\\top A y \\] and player \\(X\\) wants to maximize it, while player \\(Y\\) wants to minimize it. The point \\((x^\\ast, y^\\ast) \\in \\hat{X} \\times \\hat{Y}\\) iff the following two points hold:\n\n\\(x^\\ast\\) together with a scalar value \\(v^\\ast\\) solves the primal linear program\n\n\\[ \\max_{v,x} v \\text{\\quad s.t.\\quad} x^\\top A \\geq ve^\\top, x \\in \\hat{X}\\]\n\n\\(y^\\ast\\) together with a scalar value \\(w^\\ast\\) solve the dual linear program\n\n\\[\\min_{y,w} w \\text{\\quad s.t.\\quad} A y \\leq we, y \\in \\hat{Y}.\\] The vector \\(e = (1, ..., 1)^\\top\\) has the suitable dimension.\nThis theorem reduces the finding the Nash-equilibrium to solving a linear program which is relatively easy. Now let’s turn to a concrete calculation.\n\n\n\nWe implement a simple example given in [[1]] on p. 67 using JuMP, a Julia package for solving optimization problems. The payoff matrix is given by \\[ A = \\begin{pmatrix} 1 & 0 \\\\ -1 & 2 \\end{pmatrix} \\]. After executing the code below, it can be seen that the value of the game is \\(0.5\\) and the strategy for the first player is indeed \\(x^\\ast = (0.75, 0.25)^\\top\\).\n\nusing JuMP\nusing HiGHS\n\n# Define the payoff matrix\nA = [1 0; -1 2]\n\ndim_player1 = size(A, 1)\ndim_player2 = size(A, 2)\n\n# Solve the primary problem +++++++++++++++++++++++++++++++++++++++++++++++++++\n## Set up the model using the HiGHS solver\nmodel = Model(HiGHS.Optimizer)\n\n## Define the variables\n@variable(model, x[1:dim_player1] &gt;= 0)\n@variable(model, v)\ne1 = ones(1, dim_player1)\n\n## Define the constraints\n@constraint(model, sum(x) == 1)\n@constraint(model, transpose(x) * A .&gt;= v .* e1)\n\n## Define the objective function for the primal problem\n@objective(model, Max, v)\n\n## Solve the optimization problem\noptimize!(model)\n\n# Output of the values for the Nash equilibrium\nstrategy_player1 = [value(x[i]) for i in 1:dim_player1]\nprintln(\"Nash equlibrium strategy for player 1:\\t $strategy_player1\")\nprintln(\"Value for Player 1:\\t $(objective_value(model))\")\n\nRunning HiGHS 1.5.3 [date: 1970-01-01, git hash: 45a127b78]\nCopyright (c) 2023 HiGHS under MIT licence terms\nPresolving model\n2 rows, 2 cols, 4 nonzeros\n2 rows, 2 cols, 4 nonzeros\nPresolve : Reductions: rows 2(-1); columns 2(-1); elements 4(-3)\nSolving the presolved LP\nUsing EKK dual simplex solver - serial\n  Iteration        Objective     Infeasibilities num(sum)\n          0    -1.0000000000e+03 Ph1: 2(2000); Du: 1(1) 0s\n          2    -5.0000000000e-01 Pr: 0(0) 0s\nSolving the original LP from the solution after postsolve\nModel   status      : Optimal\nSimplex   iterations: 2\nObjective value     :  5.0000000000e-01\nHiGHS run time      :          0.00\nNash equlibrium strategy for player 1:   [0.75, 0.25]\nValue for Player 1:  0.5\n\n\n\n\n\n[1] C. Kanzow and A. Schwartz, Spieltheorie. Theorie und Verfahren zur Lösung von Nash- und verallgemeinerten Nash-Gleichgewichtsproblemen. Cham: Birkhäuser (2018; Zbl 1418.91004)"
  },
  {
    "objectID": "blog_posts/LP_zero_sum_games.html#the-theoretical-part",
    "href": "blog_posts/LP_zero_sum_games.html#the-theoretical-part",
    "title": "How to solve a zero sum game using linear programming",
    "section": "",
    "text": "My last undergraduate course in mathematics was about Game Theory, which was quite some time ago. One of the lectures covered Neumann’s Minmax-Theorem, and during this session, the lecturer drew a picture on the blackboard to illustrate how to find the Nash Equilibrium of a zero-sum game. The drawing consisted of a set of linear inequalities, and he moved his hands along the edges of a simplex. While everyone in the lecture hall, including myself, was trying to grasp the explanation, it suddenly clicked, “Isn’t this just linear programming?!?”. Well… I was thinking with mouth moving… At that moment, the entire lecture hall turned to look at me, but the lecturer simply replied with a straightforward “yes.”\nNevertheless, it took quite some time until I stumbled upon a resource that explained the connection between game theory and linear programming, or more generally, mathematical optimization. I found the following statement in [1]:\nTheorem Given a zero-sum matrix game with payoff matrix \\(A\\) is an \\(n\\times m\\) matrix with real entries and \\[ \\hat{X} := \\{ x \\in \\mathbb{R}^n \\| \\sum_{i=1}^n x_i = 1, x \\geq 0 \\}, \\\\ \\hat{Y} := \\{ y \\in \\mathbb{R}^m \\| \\sum_{i=1}^m y_i = 1, y \\geq 0\\}\\] as the sets of mixed strategies. The payoff function is for both players is given by \\[ \\theta(x,y) = x^\\top A y \\] and player \\(X\\) wants to maximize it, while player \\(Y\\) wants to minimize it. The point \\((x^\\ast, y^\\ast) \\in \\hat{X} \\times \\hat{Y}\\) iff the following two points hold:\n\n\\(x^\\ast\\) together with a scalar value \\(v^\\ast\\) solves the primal linear program\n\n\\[ \\max_{v,x} v \\text{\\quad s.t.\\quad} x^\\top A \\geq ve^\\top, x \\in \\hat{X}\\]\n\n\\(y^\\ast\\) together with a scalar value \\(w^\\ast\\) solve the dual linear program\n\n\\[\\min_{y,w} w \\text{\\quad s.t.\\quad} A y \\leq we, y \\in \\hat{Y}.\\] The vector \\(e = (1, ..., 1)^\\top\\) has the suitable dimension.\nThis theorem reduces the finding the Nash-equilibrium to solving a linear program which is relatively easy. Now let’s turn to a concrete calculation."
  },
  {
    "objectID": "blog_posts/LP_zero_sum_games.html#code-example",
    "href": "blog_posts/LP_zero_sum_games.html#code-example",
    "title": "How to solve a zero sum game using linear programming",
    "section": "",
    "text": "We implement a simple example given in [[1]] on p. 67 using JuMP, a Julia package for solving optimization problems. The payoff matrix is given by \\[ A = \\begin{pmatrix} 1 & 0 \\\\ -1 & 2 \\end{pmatrix} \\]. After executing the code below, it can be seen that the value of the game is \\(0.5\\) and the strategy for the first player is indeed \\(x^\\ast = (0.75, 0.25)^\\top\\).\n\nusing JuMP\nusing HiGHS\n\n# Define the payoff matrix\nA = [1 0; -1 2]\n\ndim_player1 = size(A, 1)\ndim_player2 = size(A, 2)\n\n# Solve the primary problem +++++++++++++++++++++++++++++++++++++++++++++++++++\n## Set up the model using the HiGHS solver\nmodel = Model(HiGHS.Optimizer)\n\n## Define the variables\n@variable(model, x[1:dim_player1] &gt;= 0)\n@variable(model, v)\ne1 = ones(1, dim_player1)\n\n## Define the constraints\n@constraint(model, sum(x) == 1)\n@constraint(model, transpose(x) * A .&gt;= v .* e1)\n\n## Define the objective function for the primal problem\n@objective(model, Max, v)\n\n## Solve the optimization problem\noptimize!(model)\n\n# Output of the values for the Nash equilibrium\nstrategy_player1 = [value(x[i]) for i in 1:dim_player1]\nprintln(\"Nash equlibrium strategy for player 1:\\t $strategy_player1\")\nprintln(\"Value for Player 1:\\t $(objective_value(model))\")\n\nRunning HiGHS 1.5.3 [date: 1970-01-01, git hash: 45a127b78]\nCopyright (c) 2023 HiGHS under MIT licence terms\nPresolving model\n2 rows, 2 cols, 4 nonzeros\n2 rows, 2 cols, 4 nonzeros\nPresolve : Reductions: rows 2(-1); columns 2(-1); elements 4(-3)\nSolving the presolved LP\nUsing EKK dual simplex solver - serial\n  Iteration        Objective     Infeasibilities num(sum)\n          0    -1.0000000000e+03 Ph1: 2(2000); Du: 1(1) 0s\n          2    -5.0000000000e-01 Pr: 0(0) 0s\nSolving the original LP from the solution after postsolve\nModel   status      : Optimal\nSimplex   iterations: 2\nObjective value     :  5.0000000000e-01\nHiGHS run time      :          0.00\nNash equlibrium strategy for player 1:   [0.75, 0.25]\nValue for Player 1:  0.5"
  },
  {
    "objectID": "blog_posts/LP_zero_sum_games.html#references",
    "href": "blog_posts/LP_zero_sum_games.html#references",
    "title": "How to solve a zero sum game using linear programming",
    "section": "",
    "text": "[1] C. Kanzow and A. Schwartz, Spieltheorie. Theorie und Verfahren zur Lösung von Nash- und verallgemeinerten Nash-Gleichgewichtsproblemen. Cham: Birkhäuser (2018; Zbl 1418.91004)"
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html",
    "href": "blog_posts/choropleth_map_brandenburg.html",
    "title": "How to create a choroplath map in R",
    "section": "",
    "text": "Now and then, you come across choropleth maps presented as animated graphics, like on /r/dataisbeautiful. Personally, I’m not so interested in the end result, but rather in the process that leads to the result. This blog post demonstrates how to create an animated GIF of a choropleth map using R.\nMost examples on the internet showcase choropleth maps of either the United States or the European Union. However, the majority of the world’s population does not reside in the United States, nor are they solely interested in international-scale data. Consequently, I’ve chosen to provide an example concerning the precipitation in the German state of Brandenburg since 2015. We will compare the average precipitation in each district to the overall mean precipitation of the reference period. As climatic patterns exhibit seasonal variations, we will calculate the monthly average to determine whether a particular month has experienced above or below average rainfall. Adhering to the convention, we will employ a 30-year period to compute climate normals as the mean and select the time frame from January 1970 to December 1999 as the reference period."
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html#step-1-data-aquisition",
    "href": "blog_posts/choropleth_map_brandenburg.html#step-1-data-aquisition",
    "title": "How to create a choroplath map in R",
    "section": "Step 1: Data aquisition",
    "text": "Step 1: Data aquisition\n\nThe geographical data\nEach German state is divided into various districts, necessitating data that provides a high-resolution representation of the administrative boundaries. The website gadm.org provides open data that suits our requirements, and their license permits its use under a CC-BY license. We acquire the geojson-file for the level 2 subdistricts from the website and then proceed to open it within R.\nThis process is quite straightforward, requiring just a few lines of code in R: 31b8e172-b470-440e-83d8-e6b185028602:dAB5AHAAZQA6AE8AUQBCAGoAQQBHAEkAQQBOAHcAQQA1AEEARwBVAEEATgBnAEIAagBBAEMAMABBAE8AQQBBADQAQQBEAGcAQQBaAEEAQQB0AEEARABRAEEAWQBRAEEAdwBBAEcAVQBBAEwAUQBBADUAQQBHAE0AQQBPAFEAQgBpAEEAQwAwAEEAWgBnAEIAbQBBAEQAWQBBAE4AdwBCAGoAQQBHAFEAQQBOAEEAQQB3AEEARABRAEEAWgBRAEEAMgBBAEQAQQBBAAoAcABvAHMAaQB0AGkAbwBuADoATQBRAEEANQBBAEQAQQBBAE8AQQBBAD0ACgBwAHIAZQBmAGkAeAA6AAoAcwBvAHUAcgBjAGUAOgBZAEEAQgBnAEEARwBBAEEAZQB3AEIANwBBAEgASQBBAGYAUQBCADkAQQBBAG8AQQBiAEEAQgBwAEEARwBJAEEAYwBnAEIAaABBAEgASQBBAGUAUQBBAG8AQQBIAE0AQQBaAGcAQQBwAEEAQwBBAEEAQwBnAEIAegBBAEcAWQBBAFgAdwBCAG4AQQBHAFUAQQBjAGcAQgB0AEEARwBFAEEAYgBnAEIANQBBAEMAQQBBAFAAQQBBAHQAQQBDAEEAQQBjAHcAQgAwAEEARgA4AEEAYwBnAEIAbABBAEcARQBBAFoAQQBBAG8AQQBDAEkAQQBMAHcAQgB3AEEARwBFAEEAZABBAEIAbwBBAEYAOABBAGQAQQBCAHYAQQBGADgAQQBaAGcAQgBwAEEARwB3AEEAWgBRAEEAdgBBAEcAYwBBAFkAUQBCAGsAQQBHADAAQQBOAEEAQQB4AEEARgA4AEEAUgBBAEIARgBBAEYAVQBBAFgAdwBBAHkAQQBDADQAQQBhAGcAQgB6AEEARwA4AEEAYgBnAEEAaQBBAEMAawBBAEMAZwBBAEsAQQBDAE0AQQBJAEEAQgB6AEEARwBVAEEAYgBBAEIAbABBAEcATQBBAGQAQQBBAGcAQQBIAFEAQQBhAEEAQgBsAEEAQwBBAEEAYwB3AEIAMABBAEcARQBBAGQAQQBCAGwAQQBDAEEAQQBRAGcAQgB5AEEARwBFAEEAYgBnAEIAawBBAEcAVQBBAGIAZwBCAGkAQQBIAFUAQQBjAGcAQgBuAEEAQwBBAEEAYgB3AEIAbQBBAEMAQQBBAFIAdwBCAGwAQQBIAEkAQQBiAFEAQgBoAEEARwA0AEEAZQBRAEEAZwBBAEcAWQBBAGMAZwBCAHYAQQBHADAAQQBJAEEAQgAwAEEARwBnAEEAWgBRAEEAZwBBAEgATQBBAGEAQQBCAGgAQQBIAEEAQQBaAFEAQgBtAEEARwBrAEEAYgBBAEIAbABBAEEAbwBBAGMAdwBCAG0AQQBGADgAQQBZAGcAQgB5AEEARwBFAEEAYgBnAEIAawBBAEcAVQBBAGIAZwBCAGkAQQBIAFUAQQBjAGcAQgBuAEEAQwBBAEEAUABBAEEAdABBAEMAQQBBAFoAZwBCAHAAQQBHAHcAQQBkAEEAQgBsAEEASABJAEEASwBBAEIAegBBAEcAWQBBAFgAdwBCAG4AQQBHAFUAQQBjAGcAQgB0AEEARwBFAEEAYgBnAEIANQBBAEMAdwBBAEkAQQBCAE8AQQBFAEUAQQBUAFEAQgBGAEEARgA4AEEATQBRAEEAZwBBAEQAMABBAFAAUQBBAGcAQQBDAEkAQQBRAGcAQgB5AEEARwBFAEEAYgBnAEIAawBBAEcAVQBBAGIAZwBCAGkAQQBIAFUAQQBjAGcAQgBuAEEAQwBJAEEASwBRAEEASwBBAEcAQQBBAFkAQQBCAGcAQQBBAD0APQAKAHMAdQBmAGYAaQB4ADoA:31b8e172-b470-440e-83d8-e6b185028602\n\n\nThe meteorological data\nThe German meteorological service, Deutscher Wetterdienst (DWD), publishes meteorological and climate data in Germany under a permissive license. This data is accessible through the DWD’s Server. While manually downloading the data is a bit cumbersome, we have opted to utilize the RDWD package to streamline the process of selecting the appropriate data. However, selecting the correct meteorological station and their corresponding ID is crucial.\nThe climate data stations are documented in a txt-file, which we read and store for all weather stations in Brandenburg using the read_table function from the readr package. This function is capable of directly reading URLs, allowing us to retrieve the data from the web without intermediaries. Yet, our interest is limited to stations with data available in the climate normal period of January 1970 to December 1999, and we apply data filtering using the dplyr package.\nFollowing the station selection, we employ the selectDWD function from the rdwd package to choose the relevant stations on the server and subsequently download them. The code for this process is relatively straightforward:\n```{r}\nlibrary(readr)\nlibrary(lubridate)\n\nclimate_normal_start &lt;- ymd(19700101)\nclimate_normal_end &lt;- ymd(19991231)\n\nstations_germany &lt;- read_table(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/monthly/kl/historical/KL_Monatswerte_Beschreibung_Stationen.txt\",  col_types = cols(Stationshoehe = col_skip(), Stationsname = col_skip()))\n\n# select the stations in Brandenburg\nstations_brandenburg &lt;- stations_germany %&gt;%\n  mutate(von_datum = ymd(von_datum), bis_datum = ymd(bis_datum)) %&gt;%\n  filter(Bundesland == \"Brandenburg\" & von_datum &lt;= climate_normal_start & bis_datum &gt;= climate_normal_end)\n\n# Download the meteorological data\nurls &lt;- selectDWD(id=stations_subdivisions$station_id, res = \"monthly\", var = \"kl\", per = \"hr\")\nmeteo_data_raw &lt;- dataDWD(urls)\nmeteo_data &lt;- bind_rows(meteo_data_raw) %&gt;%\n  mutate(month_measurement = as_date(MESS_DATUM), precipitation = MO_RR) %&gt;%\n  select(STATIONS_ID, month_measurement, precipitation) %&gt;%\n  rename(station_id = STATIONS_ID) \n```\nThe data for each station is stored individually on the server, and the dataDWD function returns a list of data frames. We use the rbind function to concatenate these data frames, creating a single data frame. Specifically, we focus on the MO_RR column, which represents precipitation measurements at each station. We select this column along with the corresponding month of measurement and the station’s unique ID.\nThe details regarding the data frame stations_subsdivision will be elaborated in the following section."
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html#step-2-select-the-district-of-each-meteorological-station",
    "href": "blog_posts/choropleth_map_brandenburg.html#step-2-select-the-district-of-each-meteorological-station",
    "title": "How to create a choroplath map in R",
    "section": "Step 2: Select the district of each meteorological station",
    "text": "Step 2: Select the district of each meteorological station\nIn the next step we have to combine the data our geographical data with our meteorological data.To accomplish this, we create a data frame that includes the station’s ID and its corresponding district information.\nThe weather station’s coordinates are provided as longitude and latitude, while the subdistricts contain a geometry column that defines the boundary of each district as a polygon. Given that the districts form a partition of Brandenburg’s surface, we can determine whether a weather station’s location falls within a particular polygon using the st_contains function.\nThis process yields a temporary data frame named stations_subdivisions, which establishes a mapping between the weather station’s ID and the corresponding district. 31b8e172-b470-440e-83d8-e6b185028602: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:31b8e172-b470-440e-83d8-e6b185028602\nIn the last step we do a left join, so our meterological data now contains the information about the station’s district."
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html#step-3-computations",
    "href": "blog_posts/choropleth_map_brandenburg.html#step-3-computations",
    "title": "How to create a choroplath map in R",
    "section": "Step 3: Computations",
    "text": "Step 3: Computations\nTo begin, we calculate the mean, standard error for each mean, and the number of stations within each district during the reference period for the climate from January 1970 to December 1999. This analysis is performed using the summarise function from the dplyr package.\nThe process is carried out using the following R code: 31b8e172-b470-440e-83d8-e6b185028602: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:31b8e172-b470-440e-83d8-e6b185028602\nThe number of stations and the standard error for the mean are just for the one, who sits in front of the screen. IThese figures facilitate a better understanding of the data and potential errors. The identical calculation is performed for the recent meteorological data, encompassing measurements since January 2015.\n```{r}\n recent_data &lt;- meteo_data %&gt;%\n  filter(month_measurement &gt;= start_recent_period) %&gt;%\n  mutate(month = month(month_measurement),\n         year = year(month_measurement)) %&gt;%\n  group_by(month, year, subdivision) %&gt;%\n  summarise(recent_mean = mean(precipitation, na.rm = TRUE), n = n())\n```\nIn the final computational step, we determine the relative precipitation \\(r_p\\) using the formula:\n\\[\nr_p = \\frac{p}{\\overline{p}}.\n\\]\nIn this equation, \\(p\\) represents precipitation, and \\(\\overline{p}\\) signifies the average precipitation. The outcome is a dimensionless value that enables a comparison between a month’s precipitation in a specific region and the reference period’s precipitation within the climate timeframe. For instance, \\(r_p = 1.1\\) indicates that the precipitation during this month was 10% higher than the average.\nThe provided R code computes the relative score and integrates it into the shapefile.\n```{r}\n# compute the relative precipitation for each district and\n# each month since Jan 2015.\nscores_subdivisions &lt;- left_join(recent_data, climate_normals) %&gt;%\n  ungroup() %&gt;%\n  mutate(relative_precipitation = round(recent_mean / mean_precipitation , 1),\n         month = make_date(year, month, 1)) %&gt;%\n  select(subdivision,month,  relative_precipitation)\n\n\nsf_brandenburg_plot &lt;- sf_brandenburg %&gt;%\n  mutate(temp_id = 1) %&gt;%\n  full_join(expand.grid(temp_id = 1, month = seq(start_recent_period, end_recent_period, by = \"month\"))) %&gt;%\n  left_join(scores_subdivisions, by = c(\"NAME_2\" = \"subdivision\", \"month\" = \"month\")) %&gt;%\n  select(-temp_id)\n```"
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html#step-4-create-frames-for-the-animation",
    "href": "blog_posts/choropleth_map_brandenburg.html#step-4-create-frames-for-the-animation",
    "title": "How to create a choroplath map in R",
    "section": "Step 4: Create frames for the animation",
    "text": "Step 4: Create frames for the animation\nWe aim to construct an animation. The typical choice for creating animations in R is the gganimate package, which, however, has numerous dependencies. Like more often than not, I encountered problems while attempting to install all of these dependencies on my Linux distribution laptop. As a workaround, I devised a solution involving the generation of each frame individually. Subsequently, I developed a small CLI program to merge these frames into a GIF file.\nThe ggplot2 package provides the geom_sf plot geometry function, streamlining the process of plotting shapefiles.\nFor every month, we craft a plot using ggplot2 and store it within a local directory named frames.\n```{r}\nend_recent_period &lt;- max(scores_subdivisions$month)\nrelative_precipitation_min &lt;- min(scores_subdivisions$relative_precipitation, na.rm = TRUE)\nrelative_precipitation_max &lt;- max(scores_subdivisions$relative_precipitation, na.rm = TRUE)\n\nfor (m in unique(sf_brandenburg_plot$month)) {\n  p &lt;- sf_brandenburg_plot %&gt;%\n    filter(month == m) %&gt;%\n    ggplot() + \n    geom_sf(aes(fill=relative_precipitation)) +\n    scale_fill_gradient2(low = \"red\", high = \"darkblue\", mid = \"green\", midpoint = 1, na.value = \"grey\", \n                        limits = c(relative_precipitation_min, relative_precipitation_max),\n                        name = \"relative precipitation\") + \n    theme_bw() +\n    labs(title=\"Deviation of the precipitation from the mean of 1970 - 1999 \\n in Brandenburg, Germany\") +\n    annotate(geom=\"label\", x = 11.9, y = 51.8, label = paste(\"Month:\", strftime(as_date(m), \"%b %Y\")), fill=\"white\") +\n    xlab(\"\") + \n    ylab(\"\")\n  \n  ggsave(paste0(\"/my_path//frames/frame-\", strftime(as_date(m), \"%Y-%m\"), \".png\"), p)\n\n}\n```"
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html#step-5-create-an-animation-from-the-frames",
    "href": "blog_posts/choropleth_map_brandenburg.html#step-5-create-an-animation-from-the-frames",
    "title": "How to create a choroplath map in R",
    "section": "Step 5: Create an animation from the frames",
    "text": "Step 5: Create an animation from the frames\nIn the last step, I coded a small python programm which uses the libraries Pillow and Python Fire to create a gif file. The source code is relative short:\n```{python}\n # Load libraries\n import os\nimport sys\nfrom PIL import Image\nimport fire\n\n# define constants\nDEFAULT_OUTPUT_PATH = \"./\"\nDOT = \".\"\nSUPPORTED_FORMATS = (\"jpeg\", \".jpeg\", \"jpg\", \".jpg\", \".png\", \"png\")\n\n# define the function to create the output gif.\ndef combine_to_gif(name:str=\"output.gif\", output_path:str=DEFAULT_OUTPUT_PATH, file_type:str=\".png\", duration:int=500, print_done:bool=False, *args):\n    if output_path == DEFAULT_OUTPUT_PATH:\n        output_path = os.getcwd()\n    \n    try:\n        if file_type not in SUPPORTED_FORMATS:\n            raise ValueError\n        if len(args) == 0:\n            args = sys.stdin.read().splitlines() # read from command line\n        if len(args) == 0:\n            raise ValueError(\"No images given\")\n        frames = [Image.open(arg) for arg in args]\n        img = frames[0]\n        file_path = output_path + \"/\" + name\n        img.save(fp=file_path, format=\"GIF\", append_images=frames,\n                 save_all=True, duration=duration, loop=0)\n        \n        if print_done:\n            print(\"Gif file created\", file_path)\n    except Exception as e:\n        print(e)\n\nif __name__ == \"__main__\":\n    fire.Fire(combine_to_gif)\n```"
  },
  {
    "objectID": "blog_posts/choropleth_map_brandenburg.html#the-result",
    "href": "blog_posts/choropleth_map_brandenburg.html#the-result",
    "title": "How to create a choroplath map in R",
    "section": "The result",
    "text": "The result\nUsing the code above yields now the following gif: \nUsing the workaround results in a lower quality of the colour, which is due to the specification of the gif file format. The full source code can be found here"
  },
  {
    "objectID": "blog_posts/arima_modeling.html",
    "href": "blog_posts/arima_modeling.html",
    "title": "Forecasting Wikipedia page views with seasonal ARIMA models",
    "section": "",
    "text": "One may say that the future is not predictable, which is true most of the time. However humans recognize repeating patterns and know that it is colder in winter than in summer and expect therefore lower temperatures in winter. To quantify those expectations is difficult, especially for complex phenomena like the weather. For less complex systems we can use ARIMA models to predict future values of time series. In this blog post I am going to explain how ARIMA models can be used to forecast page views of Wikipedia articles using R.\n\nAcquiring the data\nFirst of all we need some data to build our model and preprocess the data, so we load some packages.\n\n\nCode\n# packages for data wrangling\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(kableExtra)\n\n# packages for times series\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(zoo)\n\n# graphics\nlibrary(ggplot2)\nlibrary(scales)\n# google trends\nlibrary(gtrendsR)\n\n\nWikimedia offers a REST API to retrieve page views from various Wikimedia projects, such as the English version of Wikipedia. In order to interact with the API, I’ve written a function that handles the API requests, which you can view in the code below.\n\n\nFunction to get page views of Wikipedia articles\n#' Get Wikipedia Page Views Data\n#'\n#' Retrieve page views data for a Wikipedia article.\n#'\n#' @param article The title of the Wikipedia article.\n#' @param project The Wikipedia project (default: \"en.wikipedia\").\n#' @param access Type of access (default: \"all-access\").\n#'   Must be one of: \"all-access\", \"desktop\", \"mobile-app\", \"mobile-web\".\n#' @param agent Type of agent (default: \"all-agents\").\n#'   Must be one of: \"all-agents\", \"user\", \"spider\", \"automated\".\n#' @param granularity Data granularity (default: \"daily\").\n#'   Must be one of: \"daily\", \"monthly\".\n#' @param start Start date (in Date format) for the data retrieval.\n#' @param end End date (in Date format) for the data retrieval.\n#'\n#' @return A tibble containing the retrieved page views data.\n#'\n#' @details This function retrieves page views data for a Wikipedia article\n#' using the Wikimedia REST API.\n#'\n#' @seealso [Wikimedia REST API Documentation](https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/)\n#'\n#' @examples\n#' \\dontrun{\n#'   article_data &lt;- get_wiki_pageviews(\"Example_Article\", start = as.Date(\"2023-01-01\"), end = as.Date(\"2023-01-31\"))\n#'   print(article_data)\n#' }\n#'\n#' @import httr2\n#' @import jsonlite\n#' @import lubridate\n#' @import dplyr\n#' @importFrom glue glue\n#'\n#' @export\nget_wiki_pageviews &lt;- function(article,\n                               project = \"en.wikipedia\",\n                               access = c(\"all-access\", \"desktop\", \"mobile-app\", \"mobile-web\"),\n                               agent = c(\"all-agents\", \"user\", \"spider\", \"automated\"),\n                               granularity = c(\"daily\", \"monthly\"),\n                               start,\n                               end) {\n  # allow only specified arguments\n  access &lt;- match.arg(access)\n  agent &lt;- match.arg(agent)\n  granularity &lt;- match.arg(granularity)\n  \n  # allow only Date types for start and end\n  stopifnot(is.Date(start))\n  stopifnot(is.Date(end))\n  \n  # convert to the date format YYYYMMDD\n  start &lt;- format(start, \"%Y%m%d\")\n  end &lt;- format(end, \"%Y%m%d\")\n  \n  # create the request\n  url &lt;- glue(\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}\")\n  req = request(url)\n  # perform the request\n  wiki_response &lt;- req_perform(req)\n  \n  # convert request to a tibble if response is sucessful\n  if (wiki_response$status_code == 200) {\n    wiki_response &lt;- resp_body_json(wiki_response)$items\n    wiki_response &lt;- do.call(rbind, lapply(wiki_response, as_tibble)) %&gt;%\n      mutate(timestamp = ymd_h(timestamp))\n  } else {\n    cat(\"Error: Unable to retrieve page views data.\\n\")\n    return(NULL)\n  }\n}\n\n\nThere are a vast range of possibilities to choose a specific article. For example when a famous person dies, the number of page views can increase drastically (for example Queen Elisabeth II) or singular events happen which are covered by the media. Also, the Wikipedia page for small villages in rural areas have basically no traffic. Hence I have opted for an undergraduate topic which is taught in basically all universities around the globe: the Euler method.\nIn the first step, we retrieve the page view data from July 2015 to September 2023 and perform a train/test split. The training data consists of the data until December 2021, and the rest constitutes the test data.\nThe following code loads the page view data, converts the data into a tsibble for tidy time series objects(Wang, Cook, and Hyndman 2020) and do the train/test split.\n\nWang, Earo, Dianne Cook, and Rob J Hyndman. 2020. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Journal of Computational and Graphical Statistics 29 (3): 466–78. https://doi.org/10.1080/10618600.2019.1695624.\n\n\nCode\n# Define constants in the program\nWikipedia_article &lt;- \"Euler_method\"\nstart_period &lt;- as_date(\"2015-07-01\")\nend_period &lt;- as_date(\"2023-09-30\")\ntrain_split_date &lt;- as_date(\"2022-01-01\")\n\n# get the page views from the API for monthly data\nwiki_page_views &lt;- get_wiki_pageviews(Wikipedia_article, \n                                      agent = \"user\",\n                                      access = \"all\",\n                                      granularity = \"monthly\", \n                                      start = start_period, \n                                      end = end_period)\n\n# get the page views from the API for daily data\nwiki_page_views_daily &lt;- get_wiki_pageviews(Wikipedia_article, \n                                      agent = \"user\",\n                                      access = \"all\",\n                                      granularity = \"daily\", \n                                      start = start_period, \n                                      end = end_period)\n\n# converting to tsiblle objects\nwiki_page_views &lt;- wiki_page_views %&gt;%\n  select(timestamp, views) %&gt;%\n  mutate(timestamp = yearmonth(timestamp)) %&gt;%\n  as_tsibble(index = timestamp)\n\nwiki_page_views_daily &lt;- wiki_page_views_daily %&gt;%\n  select(timestamp, views) %&gt;%\n  mutate(timestamp = as_date(timestamp)) %&gt;%\n  as_tsibble(index = timestamp)\n\n# Doing the train / test split\ntest_data &lt;- wiki_page_views %&gt;%\n  filter(as_date(timestamp) &gt;= train_split_date)\n\ntraining_data &lt;- wiki_page_views %&gt;%\n  filter(as_date(timestamp) &lt; train_split_date)\n\n\n\n\nA first analysis of the data\n\n\nShow code for the plot\nwiki_page_views_daily %&gt;%\n  mutate(moving_avg = zoo::rollmean(views, 7, fill = NA)) %&gt;%\n  ggplot() +\n  geom_line(aes(timestamp, views), color = \"lightblue\") + \n  geom_line(aes(timestamp, moving_avg), color = \"red\") +\n  theme_light() +\n  ylim(0, 1.1 * max(wiki_page_views_daily$views)) +\n  scale_x_date(breaks = breaks_width(\"1 year\"), labels = date_format(\"%b %y\")) +\n  labs(title = paste(\"Daily Wikipedia page views:\", \n                     stringr::str_replace(Wikipedia_article, \"_\", \" \"))) + \n  xlab(\"Date\") + \n  ylab(\"Page views per day\")\n\n\n\n\n\nDaily page views of the Wikipedia article for the Euler Method. The daily values are in light blue and the 7 day moving average in red.\n\n\n\n\nFirst, let’s take a look at the figure above, which displays the daily page views starting from July 2015. There is considerable variability in the data, making it challenging to interpret. To enhance data readability, we’ve also included a 7-day moving average plot. We can make two oberservations:\n\nThe year 2017 has significantly more page views than the rest of the data. This observation will likely be seen in the test, since it is in the training data.\nThere are recurring seasonal patterns, such as a sharp drop in the last week of the year, which is likely related to Christmas. Additionally, there are drops in March and during the summer, possibly because university students are less likely to read math articles on Wikipedia when there are no lectures during these times.\n\nUnfortunately, ARIMA models don’t work well with daily data, where there is a seasonal period of 365 days (see R. Hyndman and Athanasopoulos 2021, 10.5 Dynamic harmonic regression). Furthermore the number of page views depends on the day of the week as an ANOVA shows (see the margin note on the right), which complicates the modelling process.\n\nHyndman, Rob, and G. Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. Australia: OTexts.\n\n\nCode for ANOVA\nwiki_page_views_daily %&gt;%\n  mutate(weekday = weekdays(timestamp)) %&gt;%\n  select(views, weekday) %&gt;%\n  aov(views ~ weekday, data = .) %&gt;%\n  summary()\n\n\n              Df    Sum Sq Mean Sq F value Pr(&gt;F)    \nweekday        6  52242548 8707091   126.3 &lt;2e-16 ***\nResiduals   3007 207380772   68966                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTherefore, we focus on monthly values and first plot the monthly page views for the article in question.\n\n\nShow code for the plot\nwiki_page_views %&gt;%\n  mutate(timestamp = as_date(timestamp)) %&gt;%  # date class is required for ggplot\n  ggplot() +\n  geom_line(aes(timestamp, views), color = \"blue\") + \n  theme_light() +\n  ylim(0, 1.1 * max(wiki_page_views$views)) +\n  scale_x_date(breaks = breaks_width(\"2 year\"), labels = date_format(\"%b %y\")) +\n  scale_y_continuous(labels = label_number()) +\n  labs(title = paste(\"Monthly Wikipedia page views:\", \n                     stringr::str_replace(Wikipedia_article, \"_\", \" \"))) + \n  xlab(\"Date\") + \n  ylab(\"Page views per month\")\n\n\n\n\n\nPage views of the Wikipedia article for the Euler Method. The monthly values are in blue.\n\n\n\n\nThe monthly observations are just the addition of the daily values, however several effects are disguised and are not so obvious like before, such as the effect of Christmas or the number of weekdays, which influence the number of views per month. The decline can also be seen more clearly in the monthly data as it reduces the daily variability.\n\n\nActually it is possible calculate the trend of a seasonal time series. This is called seasonal adjustment.\n\n\nShow code for the plot\nwiki_page_views %&gt;%\n  gg_season(views) + \n  theme_light() +\n  labs(title = \"Seasonal plot for pages views\") +\n  ylim(0, 1.1 * max(wiki_page_views$views)) +\n  xlab(\"Month\") + \n  ylab(\"number of page views\")\n\n\n\n\n\nSeasonal plot for the page views. For each year the page views from January to December are plotted.\n\n\n\n\nWith this perspective it is possible to see the seasonal pattern more clearly in figure above. Nevertheless, some of the effects are dampened, like the Christmas effect. The autocorrelation function (which is simply the Pearson correlation coefficient between the lags) in the figure below shows a high correlation after one lag (\\(\\rho_1 \\approx 0.7\\)) and after 12 months (\\(\\rho\\_{12} \\approx 0.53\\)), which is the seasonality of our time series.\n\n\nShow code for the plot\nwiki_page_views %&gt;%\n  ACF(views) %&gt;%\n  autoplot() +\n  theme_light()\n\n\n\n\n\nAutocorrelation plot of the data\n\n\n\n\n\n\nARIMA Modeling\nIn order to calculate our forecast we use a seasonal ARIMA-model. An good introduction is Shumway and Stoffer (2006), where the following part is adapted from. The back casting operator \\(B^k\\) is defined as\n\\[\nB^k x_t = x_{t-k}\n\\]\nfor \\(k \\in \\mathbb{N}\\). This leads straightforward to the difference operator\n\\[\n\\nabla^d x_t = (1- B)^d x_t\n\\]\nand the seasonal difference operator\n\\[\n\\nabla_s^Dx_t = (1 - B^s)^D x_t.\n\\]\nThe order of the difference \\(D\\) or \\(d\\) is a non-negative integer. The superscript indicates the repeated use of the operator, e.g. \\(\\nabla^{d+1} = \\nabla \\circ\\nabla^d\\). Using this can be plugged into an ARMA with\n\\[\n\\Phi_P(B^s)\\phi_p(B)\\nabla_s^D\\nabla^d x_t = \\alpha + \\Theta_Q(B^s)\\theta_q(B) \\omega_t\n\\]\nwhere \\(\\Phi, \\Theta, \\phi, \\theta\\) are formal polynomials of the respective operator and the subscript denotes the order of the polynomial and \\(\\alpha \\in \\mathbb{R}\\) and \\(\\omega_t\\) is Gaussian white noise. This model is called seasonal autoregressive integrated moving average model or short SARIMA. The model is usually denoted by \\(ARIMA(p,d,q)\\times (P,D,Q)_s\\).\nFitting an SARIMA model is not easy (perhaps I will explain it in another blog post). The fable package (see R. J. Hyndman and Khandakar 2008) in R makes it easy for the user to handle the modeling, especially since it searches the model space for the model with the lowest AIC. However it still requires knowledge from the person modeling it.\n\nHyndman, Rob J., and Yeasmin Khandakar. 2008. “Automatic Time Series Forecasting: The Forecast Package for r.” Journal of Statistical Software 27 (3): 1–22. https://doi.org/10.18637/jss.v027.i03.\nWe try 3 different models and test them, namely:\n\nWithout transforming the values,\ntaking the logarithm of the page views and,\nusing the Box-Cox-Transform after using the Guerrero-method (Guerrero 1993) for determining \\(\\lambda\\).\n\n\nGuerrero, Victor M. 1993. “Time-Series Analysis Supported by Power Transformations.” Journal of Forecasting 12 (1): 37–48. https://doi.org/https://doi.org/10.1002/for.3980120104.\n\n\nCode\n# Find \\lambda using the Guerrero\nlambda &lt;- training_data %&gt;%\n  features(views, features = guerrero) %&gt;%\n  pull(lambda_guerrero)\n\n# model the different ARIMA models\nsarima &lt;- training_data %&gt;%\n  model(log_arima = ARIMA(log(views) ~ 1 + pdq() + PDQ()),\n        box_cox_arima = ARIMA(box_cox(views, lambda) ~ 1 + pdq() + PDQ()),\n        arima = ARIMA(views ~ 1 + pdq() + PDQ())) \n\n# Output the different models\nreport(sarima) \n\n\n# A tibble: 3 × 8\n  .model         sigma2 log_lik    AIC   AICc    BIC ar_roots   ma_roots \n  &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;   \n1 log_arima     1.48e-2    43.8  -79.7  -79.0  -70.9 &lt;cpl [13]&gt; &lt;cpl [0]&gt;\n2 box_cox_arima 5.57e-6   312.  -616.  -615.  -607.  &lt;cpl [13]&gt; &lt;cpl [0]&gt;\n3 arima         1.04e+7  -629.  1268.  1269.  1279.  &lt;cpl [25]&gt; &lt;cpl [0]&gt;\n\n\nCode\n# compute the forecast\nforecasted_values &lt;- forecast(sarima, h = \"21 months\")\n\n\nWe compare the different forecasts with our test data with the accuracy function from fabletools and observe that the ARIMA model without transforming the values works best. Hence we choose this model, however each of these models exhibits a bias, since the mean error is not even close to zero.\n\n\nCode\naccuracy(forecasted_values, test_data) %&gt;% select(-c(\"MASE\", \"RMSSE\")) %&gt;% kable()\n\n\n\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nACF1\n\n\n\n\narima\nTest\n-1205.332\n1842.091\n1480.995\n-5.177114\n6.678774\n0.3347987\n\n\nbox_cox_arima\nTest\n-2360.290\n2896.806\n2449.680\n-10.827055\n11.268639\n0.4705629\n\n\nlog_arima\nTest\n-1840.481\n2369.081\n1998.892\n-8.466518\n9.250628\n0.3985929\n\n\n\n\n\n\n\nWe can now output the model and inspect the residuals. We notice that those are approximately normally distributed except for the outlier in 2017. However there is no significant autocorrelation and the best model for this time series is an \\(ARIMA(1,0,0)\\times(2,1,0)_{12}\\) model\n\n\nCode for post processing the data\n# Output the model\nsarima %&gt;%\n  select(arima) %&gt;%\n  report()\n\n\nSeries: views \nModel: ARIMA(1,0,0)(2,1,0)[12] w/ drift \n\nCoefficients:\n         ar1     sar1     sar2  constant\n      0.7562  -0.7920  -0.2522  -87.4229\ns.e.  0.0812   0.1247   0.1520  415.1631\n\nsigma^2 estimated as 10407778:  log likelihood=-629.05\nAIC=1268.11   AICc=1269.11   BIC=1279.05\n\n\nCode for post processing the data\n# Inspect the residuals\nsarima %&gt;%\n  select(arima) %&gt;%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow code for the plot\nforecasted_values %&gt;%\n  filter(.model == \"arima\") %&gt;%\n  autoplot() +\n  geom_line(data = wiki_page_views, mapping = aes(timestamp, views), color = \"red\") +\n  theme_light() +\n  labs(title = \"Forecasted values vs. test data\") +\n  ylab(\"page views\") +\n  xlab(\"date\")\n\n\n\n\n\nForecasted values with confidence intervals in blue and true values in red\n\n\n\n\nWhen we plot our forecast we see that the already mentioned bias, which overestimates the page views. It seems like that the declining trend is not captured very well. The spring high in 2022 was estimated to high and the impact of December is not so good modeled by this model. However it estimates the page views with a mean error of 6% which is quite good for an off the shelve method.\n\n\nWhy not using external data?\nAt this point, an interested reader might ask, why don’t we use an external data source as an predictor. The fable package supports ARIMAX-Models, which enable us to use exogenous data (that’s why the name ARIMAX). For more information on ARIMAX-models see also Shumway and Stoffer (2006) as a reference.\n\nShumway, Robert H., and David S. Stoffer. 2006. Time Series Analysis and Its Applications. With R Examples. 2nd ed. New York, NY: Springer.\nAn external data source could be Google trends, since Google has a huge market share on search engines. The package gtrends allows an easy access to via R.\n\n\nShow code for Google trends\ngoogle_trend_euler_method &lt;- gtrends(\"euler method\", time = paste(as.character(start_period), as.character(end_period)))\n\ngoogle_trend_euler_method &lt;- google_trend_euler_method$interest_over_time %&gt;%\n  select(date, hits) %&gt;%\n  mutate(date = yearmonth(date)) %&gt;%\n  tsibble(index = date)\n\nwiki_google_trend &lt;- inner_join(google_trend_euler_method, wiki_page_views, by = c(\"date\" = \"timestamp\"))\n\nrho &lt;- cor(wiki_google_trend$views, wiki_google_trend$hits)\n\nwiki_google_trend %&gt;%\n  ggplot() +\n  geom_line(aes(date, views / max(views), color = \"Wikipedia page views\")) +\n  geom_line(aes(date, hits / max(hits), color = \"Google trends\")) +\n  theme_light() +\n  theme(axis.text.y=element_blank()) +\n  labs(title = \"Google trends and Wikipedia for the term 'Euler Method'\", color = \"Source\") +\n  xlab(\"Month\") +\n  ylab(\"Accesses\")\n\n\n\n\n\nSince Google Trends provides indices where the maximum is indicated by the value 100, I rescaled both data sets to have values between 0 and 1. Until 2021 it is a good approximation, however after 2022 it becomes less accurate. The correlation coefficient is \\(\\rho = 0.738\\), which is quite high. But if we plug it into our ARIMA model, floating point errors render the model unusable."
  },
  {
    "objectID": "blog_posts/arima_modeling.html#references",
    "href": "blog_posts/arima_modeling.html#references",
    "title": "Forecasting Wikipedia page views with seasonal ARIMA models",
    "section": "References",
    "text": "References"
  }
]