geom_line(aes(date, views / max(views), color = "Wikipedia page views")) +
geom_line(aes(date, hits / max(hits), color = "Google trends"))
wiki_google_trend %>%
as
wiki_google_trend %>%
ggplot() +
geom_line(aes(date, views / max(views), color = "Wikipedia page views")) +
geom_line(aes(date, hits / max(hits), color = "Google trends")) +
theme_light()
cov(wiki_google_trend$hits, wiki_google_trend$views)
cov(wiki_google_trend$hits, wiki_google_trend$views, method = "pearson")
cor(wiki_google_trend$hits, wiki_google_trend$views, method = "pearson")
wiki_google_trend %>%
ggplot() +
geom_line(aes(date, views / max(views), color = "Wikipedia page views")) +
geom_line(aes(date, hits / max(hits), color = "Google trends")) +
theme_light() +
theme(axis.text.y=element_blank())
wiki_google_trend %>%
ggplot() +
geom_line(aes(date, views / max(views), color = "Wikipedia page views")) +
geom_line(aes(date, hits / max(hits), color = "Google trends")) +
theme_light() +
theme(axis.text.y=element_blank()) +
labs(title = "Google trends and Wikipedia for the term 'Euler Method'", fill = "Source") +
xlab("Month") +
ylab("Accesses")
wiki_google_trend %>%
ggplot() +
geom_line(aes(date, views / max(views), color = "Wikipedia page views")) +
geom_line(aes(date, hits / max(hits), color = "Google trends")) +
theme_light() +
theme(axis.text.y=element_blank()) +
labs(title = "Google trends and Wikipedia for the term 'Euler Method'", color = "Source") +
xlab("Month") +
ylab("Accesses")
rho <- cor(wiki_google_trend$views, wiki_google_trend$hits)
#| echo: false
#| eval: true
#| output: false
#| warning: false
#| code-fold: true
# packages for data wrangling
library(lubridate)
library(dplyr)
library(httr2)
library(jsonlite)
library(glue)
library(kableExtra)
# packages for times series
library(feasts)
library(tsibble)
library(fable)
library(zoo)
# graphics
library(ggplot2)
library(scales)
# google trends
library(gtrendsR)
#| eval: true
#| warning: false
#| code-fold: true
#| code-summary: Function to get page views of Wikipedia articles
#' Get Wikipedia Page Views Data
#'
#' Retrieve page views data for a Wikipedia article.
#'
#' @param article The title of the Wikipedia article.
#' @param project The Wikipedia project (default: "en.wikipedia").
#' @param access Type of access (default: "all-access").
#'   Must be one of: "all-access", "desktop", "mobile-app", "mobile-web".
#' @param agent Type of agent (default: "all-agents").
#'   Must be one of: "all-agents", "user", "spider", "automated".
#' @param granularity Data granularity (default: "daily").
#'   Must be one of: "daily", "monthly".
#' @param start Start date (in Date format) for the data retrieval.
#' @param end End date (in Date format) for the data retrieval.
#'
#' @return A tibble containing the retrieved page views data.
#'
#' @details This function retrieves page views data for a Wikipedia article
#' using the Wikimedia REST API.
#'
#' @seealso [Wikimedia REST API Documentation](https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/)
#'
#' @examples
#' \dontrun{
#'   article_data <- get_wiki_pageviews("Example_Article", start = as.Date("2023-01-01"), end = as.Date("2023-01-31"))
#'   print(article_data)
#' }
#'
#' @import httr2
#' @import jsonlite
#' @import lubridate
#' @import dplyr
#' @importFrom glue glue
#'
#' @export
get_wiki_pageviews <- function(article,
project = "en.wikipedia",
access = c("all-access", "desktop", "mobile-app", "mobile-web"),
agent = c("all-agents", "user", "spider", "automated"),
granularity = c("daily", "monthly"),
start,
end) {
# allow only specified arguments
access <- match.arg(access)
agent <- match.arg(agent)
granularity <- match.arg(granularity)
# allow only Date types for start and end
stopifnot(is.Date(start))
stopifnot(is.Date(end))
# convert to the date format YYYYMMDD
start <- format(start, "%Y%m%d")
end <- format(end, "%Y%m%d")
# create the request
url <- glue("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}")
req = request(url)
# perform the request
wiki_response <- req_perform(req)
# convert request to a tibble if response is sucessful
if (wiki_response$status_code == 200) {
wiki_response <- resp_body_json(wiki_response)$items
wiki_response <- do.call(rbind, lapply(wiki_response, as_tibble)) %>%
mutate(timestamp = ymd_h(timestamp))
} else {
cat("Error: Unable to retrieve page views data.\n")
return(NULL)
}
}
#| code-fold: true
# Define constants in the program
Wikipedia_article <- "Euler_method"
start_period <- as_date("2015-07-01")
end_period <- as_date("2023-09-30")
train_split_date <- as_date("2022-01-01")
# get the page views from the API for monthly data
wiki_page_views <- get_wiki_pageviews(Wikipedia_article,
agent = "user",
access = "all",
granularity = "monthly",
start = start_period,
end = end_period)
# get the page views from the API for daily data
wiki_page_views_daily <- get_wiki_pageviews(Wikipedia_article,
agent = "user",
access = "all",
granularity = "daily",
start = start_period,
end = end_period)
# converting to tsiblle objects
wiki_page_views <- wiki_page_views %>%
select(timestamp, views) %>%
mutate(timestamp = yearmonth(timestamp)) %>%
as_tsibble(index = timestamp)
wiki_page_views_daily <- wiki_page_views_daily %>%
select(timestamp, views) %>%
mutate(timestamp = as_date(timestamp)) %>%
as_tsibble(index = timestamp)
# Doing the train / test split
test_data <- wiki_page_views %>%
filter(as_date(timestamp) >= train_split_date)
training_data <- wiki_page_views %>%
filter(as_date(timestamp) < train_split_date)
#| warning: false
#| code-fold: true
#| code-overflow: scroll
#| code-summary: "Show code for the plot"
#| label: figDaily
#| fig-cap: "Daily page views of the Wikipedia article for the Euler Method. The daily values are in light blue and the 7 day moving average in red."
wiki_page_views_daily %>%
mutate(moving_avg = zoo::rollmean(views, 7, fill = NA)) %>%
ggplot() +
geom_line(aes(timestamp, views), color = "lightblue") +
geom_line(aes(timestamp, moving_avg), color = "red") +
theme_light() +
ylim(0, 1.1 * max(wiki_page_views_daily$views)) +
scale_x_date(breaks = breaks_width("1 year"), labels = date_format("%b %y")) +
labs(title = paste("Daily Wikipedia page views:",
stringr::str_replace(Wikipedia_article, "_", " "))) +
xlab("Date") +
ylab("Page views per day")
#| code-fold: true
#| code-summary: Code for ANOVA
#| column: margin
wiki_page_views_daily %>%
mutate(weekday = weekdays(timestamp)) %>%
select(views, weekday) %>%
aov(views ~ weekday, data = .) %>%
summary()
#| label: figMonthly
#| code-fold: true
#| code-summary: "Show code for the plot"
#| fig-cap: "Page views of the Wikipedia article for the Euler Method. The monthly values are in blue."
#| warning: false
wiki_page_views %>%
mutate(timestamp = as_date(timestamp)) %>%  # date class is required for ggplot
ggplot() +
geom_line(aes(timestamp, views), color = "blue") +
theme_light() +
ylim(0, 1.1 * max(wiki_page_views$views)) +
scale_x_date(breaks = breaks_width("2 year"), labels = date_format("%b %y")) +
scale_y_continuous(labels = label_number()) +
labs(title = paste("Monthly Wikipedia page views:",
stringr::str_replace(Wikipedia_article, "_", " "))) +
xlab("Date") +
ylab("Page views per month")
#| label: figSeason
#| fig-cap: "Seasonal plot for the page views. For each year the page views from January to December are plotted."
#| code-fold: true
#| code-summary: "Show code for the plot"
wiki_page_views %>%
gg_season(views) +
theme_light() +
labs(title = "Seasonal plot for pages views") +
ylim(0, 1.1 * max(wiki_page_views$views)) +
xlab("Month") +
ylab("number of page views")
#| label: figACF
#| fig-cap: "Autocorrelation plot of the data"
#| code-fold: true
#| code-summary: "Show code for the plot"
wiki_page_views %>%
ACF(views) %>%
autoplot() +
theme_light()
# Find \lambda using the Guerrero
lambda <- training_data %>%
features(views, features = guerrero) %>%
pull(lambda_guerrero)
# model the different ARIMA models
sarima <- training_data %>%
model(log_arima = ARIMA(log(views) ~ 1 + pdq() + PDQ()),
box_cox_arima = ARIMA(box_cox(views, lambda) ~ 1 + pdq() + PDQ()),
arima = ARIMA(views ~ 1 + pdq() + PDQ()))
# Output the different models
report(sarima)
# compute the forecast
forecasted_values <- forecast(sarima, h = "21 months")
accuracy(forecasted_values, test_data) %>% select(-c("MASE", "RMSSE")) %>% kable()
#| code-fold: true
#| code-summary: "Code for post processing the data"
# Output the model
sarima %>%
select(arima) %>%
report()
# Inspect the residuals
sarima %>%
select(arima) %>%
gg_tsresiduals()
#| code-fold: true
#| code-summary: "Show code for the plot"
#| label: "figTest"
#| fig-cap: "Forecasted values with confidence intervals in blue and true values in red"
forecasted_values %>%
filter(.model == "arima") %>%
autoplot() +
geom_line(data = wiki_page_views, mapping = aes(timestamp, views), color = "red") +
theme_light() +
labs(title = "Forecasted values vs. test data") +
ylab("page views") +
xlab("date")
#| code-fold: true
#| code-summary: "Show code for Google trends"
google_trend_euler_method <- gtrends("euler method", time = paste(as.character(start_period), as.character(end_period)))
google_trend_euler_method <- google_trend_euler_method$interest_over_time %>%
select(date, hits) %>%
mutate(date = yearmonth(date)) %>%
tsibble(index = date)
wiki_google_trend <- inner_join(google_trend_euler_method, wiki_page_views, by = c("date" = "timestamp"))
rho <- cor(wiki_google_trend$views, wiki_google_trend$hits)
wiki_google_trend %>%
ggplot() +
geom_line(aes(date, views / max(views), color = "Wikipedia page views")) +
geom_line(aes(date, hits / max(hits), color = "Google trends")) +
theme_light() +
theme(axis.text.y=element_blank()) +
labs(title = "Google trends and Wikipedia for the term 'Euler Method'", color = "Source") +
xlab("Month") +
ylab("Accesses")
#| code-fold: true
#| code-summary: "Show code for Google trends"
google_trend_euler_method <- gtrends("euler method", time = paste(as.character(start_period), as.character(end_period)))
#| eval: true
#| warning: false
#| code-fold: true
# packages for data wrangling
library(lubridate)
library(dplyr)
library(httr2)
library(jsonlite)
library(glue)
library(kableExtra)
# packages for times series
library(feasts)
library(tsibble)
library(fable)
library(zoo)
# graphics
library(ggplot2)
library(scales)
# google trends
library(gtrendsR)
#| eval: true
#| warning: false
#| code-fold: true
#| code-summary: Function to get page views of Wikipedia articles
#' Get Wikipedia Page Views Data
#'
#' Retrieve page views data for a Wikipedia article.
#'
#' @param article The title of the Wikipedia article.
#' @param project The Wikipedia project (default: "en.wikipedia").
#' @param access Type of access (default: "all-access").
#'   Must be one of: "all-access", "desktop", "mobile-app", "mobile-web".
#' @param agent Type of agent (default: "all-agents").
#'   Must be one of: "all-agents", "user", "spider", "automated".
#' @param granularity Data granularity (default: "daily").
#'   Must be one of: "daily", "monthly".
#' @param start Start date (in Date format) for the data retrieval.
#' @param end End date (in Date format) for the data retrieval.
#'
#' @return A tibble containing the retrieved page views data.
#'
#' @details This function retrieves page views data for a Wikipedia article
#' using the Wikimedia REST API.
#'
#' @seealso [Wikimedia REST API Documentation](https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/)
#'
#' @examples
#' \dontrun{
#'   article_data <- get_wiki_pageviews("Example_Article", start = as.Date("2023-01-01"), end = as.Date("2023-01-31"))
#'   print(article_data)
#' }
#'
#' @import httr2
#' @import jsonlite
#' @import lubridate
#' @import dplyr
#' @importFrom glue glue
#'
#' @export
get_wiki_pageviews <- function(article,
project = "en.wikipedia",
access = c("all-access", "desktop", "mobile-app", "mobile-web"),
agent = c("all-agents", "user", "spider", "automated"),
granularity = c("daily", "monthly"),
start,
end) {
# allow only specified arguments
access <- match.arg(access)
agent <- match.arg(agent)
granularity <- match.arg(granularity)
# allow only Date types for start and end
stopifnot(is.Date(start))
stopifnot(is.Date(end))
# convert to the date format YYYYMMDD
start <- format(start, "%Y%m%d")
end <- format(end, "%Y%m%d")
# create the request
url <- glue("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}")
req = request(url)
# perform the request
wiki_response <- req_perform(req)
# convert request to a tibble if response is sucessful
if (wiki_response$status_code == 200) {
wiki_response <- resp_body_json(wiki_response)$items
wiki_response <- do.call(rbind, lapply(wiki_response, as_tibble)) %>%
mutate(timestamp = ymd_h(timestamp))
} else {
cat("Error: Unable to retrieve page views data.\n")
return(NULL)
}
}
#| code-fold: true
# Define constants in the program
Wikipedia_article <- "Euler_method"
start_period <- as_date("2015-07-01")
end_period <- as_date("2023-09-30")
train_split_date <- as_date("2022-01-01")
# get the page views from the API for monthly data
wiki_page_views <- get_wiki_pageviews(Wikipedia_article,
agent = "user",
access = "all",
granularity = "monthly",
start = start_period,
end = end_period)
# get the page views from the API for daily data
wiki_page_views_daily <- get_wiki_pageviews(Wikipedia_article,
agent = "user",
access = "all",
granularity = "daily",
start = start_period,
end = end_period)
# converting to tsiblle objects
wiki_page_views <- wiki_page_views %>%
select(timestamp, views) %>%
mutate(timestamp = yearmonth(timestamp)) %>%
as_tsibble(index = timestamp)
wiki_page_views_daily <- wiki_page_views_daily %>%
select(timestamp, views) %>%
mutate(timestamp = as_date(timestamp)) %>%
as_tsibble(index = timestamp)
# Doing the train / test split
test_data <- wiki_page_views %>%
filter(as_date(timestamp) >= train_split_date)
training_data <- wiki_page_views %>%
filter(as_date(timestamp) < train_split_date)
#| warning: false
#| code-fold: true
#| code-overflow: scroll
#| code-summary: "Show code for the plot"
#| label: figDaily
#| fig-cap: "Daily page views of the Wikipedia article for the Euler Method. The daily values are in light blue and the 7 day moving average in red."
wiki_page_views_daily %>%
mutate(moving_avg = zoo::rollmean(views, 7, fill = NA)) %>%
ggplot() +
geom_line(aes(timestamp, views), color = "lightblue") +
geom_line(aes(timestamp, moving_avg), color = "red") +
theme_light() +
ylim(0, 1.1 * max(wiki_page_views_daily$views)) +
scale_x_date(breaks = breaks_width("1 year"), labels = date_format("%b %y")) +
labs(title = paste("Daily Wikipedia page views:",
stringr::str_replace(Wikipedia_article, "_", " "))) +
xlab("Date") +
ylab("Page views per day")
#| code-fold: true
#| code-summary: Code for ANOVA
#| column: margin
wiki_page_views_daily %>%
mutate(weekday = weekdays(timestamp)) %>%
select(views, weekday) %>%
aov(views ~ weekday, data = .) %>%
summary()
#| label: figMonthly
#| code-fold: true
#| code-summary: "Show code for the plot"
#| fig-cap: "Page views of the Wikipedia article for the Euler Method. The monthly values are in blue."
#| warning: false
wiki_page_views %>%
mutate(timestamp = as_date(timestamp)) %>%  # date class is required for ggplot
ggplot() +
geom_line(aes(timestamp, views), color = "blue") +
theme_light() +
ylim(0, 1.1 * max(wiki_page_views$views)) +
scale_x_date(breaks = breaks_width("2 year"), labels = date_format("%b %y")) +
scale_y_continuous(labels = label_number()) +
labs(title = paste("Monthly Wikipedia page views:",
stringr::str_replace(Wikipedia_article, "_", " "))) +
xlab("Date") +
ylab("Page views per month")
#| label: figSeason
#| fig-cap: "Seasonal plot for the page views. For each year the page views from January to December are plotted."
#| code-fold: true
#| code-summary: "Show code for the plot"
wiki_page_views %>%
gg_season(views) +
theme_light() +
labs(title = "Seasonal plot for pages views") +
ylim(0, 1.1 * max(wiki_page_views$views)) +
xlab("Month") +
ylab("number of page views")
#| label: figACF
#| fig-cap: "Autocorrelation plot of the data"
#| code-fold: true
#| code-summary: "Show code for the plot"
wiki_page_views %>%
ACF(views) %>%
autoplot() +
theme_light()
# Find \lambda using the Guerrero
lambda <- training_data %>%
features(views, features = guerrero) %>%
pull(lambda_guerrero)
# model the different ARIMA models
sarima <- training_data %>%
model(log_arima = ARIMA(log(views) ~ 1 + pdq() + PDQ()),
box_cox_arima = ARIMA(box_cox(views, lambda) ~ 1 + pdq() + PDQ()),
arima = ARIMA(views ~ 1 + pdq() + PDQ()))
# Output the different models
report(sarima)
# compute the forecast
forecasted_values <- forecast(sarima, h = "21 months")
accuracy(forecasted_values, test_data) %>% select(-c("MASE", "RMSSE")) %>% kable()
#| code-fold: true
#| code-summary: "Code for post processing the data"
# Output the model
sarima %>%
select(arima) %>%
report()
# Inspect the residuals
sarima %>%
select(arima) %>%
gg_tsresiduals()
#| code-fold: true
#| code-summary: "Show code for the plot"
#| label: "figTest"
#| fig-cap: "Forecasted values with confidence intervals in blue and true values in red"
forecasted_values %>%
filter(.model == "arima") %>%
autoplot() +
geom_line(data = wiki_page_views, mapping = aes(timestamp, views), color = "red") +
theme_light() +
labs(title = "Forecasted values vs. test data") +
ylab("page views") +
xlab("date")
#| code-fold: true
#| code-summary: "Show code for Google trends"
google_trend_euler_method <- gtrends("euler method", time = paste(as.character(start_period), as.character(end_period)))
#| code-fold: true
#| code-summary: "Show code for Google trends"
google_trend_euler_method <- gtrends("euler method", time = paste(as.character(start_period), as.character(end_period)))
